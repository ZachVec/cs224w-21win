{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **CS224W - Colab 3**\n",
        "\n",
        "在 Colab2 中，我们用 `PyG` 搭建了一个GNN模型。在本次实验中，我们将实现**GraphSAGE** ([Hamilton et al. (2017)](https://arxiv.org/abs/1706.02216)) 和 **GAT** ([Veličković et al. (2018)](https://arxiv.org/abs/1710.10903)) 网络。然后在 CORA 数据集上跑我们的模型，CORA 数据集是一个标准引用网络评测数据集。\n",
        "\n",
        "然后，我们将在不同的设置下分割图并对数据集加以变化，其中会使用到 Python 用来帮助进行高性能图深度学习的库——[DeepSNAP](https://snap.stanford.edu/deepsnap/) 库。\n",
        "\n",
        "最后，使用 DeepSNAP 的直推式连接预测分割功能，我们搭建了一个用来进行连接预测任务的简单 GNN 模型。\n",
        "\n",
        "**注意**：确保顺序执行所有代码单元，否则变量或库无法在之后的单元中使用。实验愉快 ：）\n",
        "\n",
        "> 在实验开始之前，请确保正确安装了 [PyG](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html) 以及 [DeepSNAP](https://snap.stanford.edu/deepsnap/notes/installation.html) 第三方库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRfgbfTjCRD_"
      },
      "outputs": [],
      "source": [
        "import torch_geometric\n",
        "torch_geometric.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoXlf4MtYrbz"
      },
      "source": [
        "## 1 GNN 层\n",
        "\n",
        "在 Colab2 中，我们实现了用来进行结点和图分类任务的模型。但是，其中用到的 `GCNConv` 是官方库中提供的模块。在本实验中，我们会提供格尼一个通用的GNN堆叠框架，你可以将你自己实现的 `GraphSAGE` 模块和 `GAT` 模块插入。我们会使用最终实现来在 CORA 数据集上进行结点分类测试。在这个数据集中，结点是文档，边是无向引用。每个结点都有一个类标签。结点特征是文档的 Bag of Words 表征。在 Cora 数据集中，一共有2708个结点、5429条边，以及7个结点类别，并且每个结点都有1433维特征。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ne6Gw-CT5G"
      },
      "source": [
        "### GNN 堆叠模块\n",
        "\n",
        "下面是可以插入任何层的通用GNN模块，包括 **GraphSage** 和 **GAT** 等。你只需要将自己的 **GraphSage** 和 **GAT** 模块作为组件插入其中即可。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys8vZAFPCWWe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_scatter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "from torch import Tensor\n",
        "from typing import Union, Tuple, Optional\n",
        "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
        "                                    OptTensor)\n",
        "\n",
        "from torch.nn import Parameter, Linear\n",
        "from torch_sparse import SparseTensor, set_diag\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
        "\n",
        "class GNNStack(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
        "        super(GNNStack, self).__init__()\n",
        "        conv_model = self.build_conv_model(args.model_type)\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
        "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
        "        for l in range(args.num_layers-1):\n",
        "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout), \n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.num_layers = args.num_layers\n",
        "\n",
        "        self.emb = emb\n",
        "\n",
        "    def build_conv_model(self, model_type):\n",
        "        if model_type == 'GraphSage':\n",
        "            return GraphSage\n",
        "        elif model_type == 'GAT':\n",
        "            # When applying GAT with num heads > 1, one needs to modify the \n",
        "            # input and output dimension of the conv layers (self.convs),\n",
        "            # to ensure that the input dim of the next layer is num heads\n",
        "            # multiplied by the output dim of the previous layer.\n",
        "            # HINT: In case you want to play with multiheads, you need to change the for-loop when builds up self.convs to be\n",
        "            # self.convs.append(conv_model(hidden_dim * num_heads, hidden_dim)), \n",
        "            # and also the first nn.Linear(hidden_dim * num_heads, hidden_dim) in post-message-passing.\n",
        "            return GAT\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "          \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "\n",
        "        if self.emb == True:\n",
        "            return x\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syDtxjxoCZgq"
      },
      "source": [
        "### 实现 GraphSage\n",
        "\n",
        "现在开始实现我们自己的网络层！这一部分可以让你熟悉一下如何基于消息传递（Message Passing）实现网络层。你将实现 `forward`、`message` 以及 `aggregate` 方法。\n",
        "\n",
        "总的来说，`forward` 方法是信息传递发生的地方。在一次迭代内的所有东西都发生在 `forward` 里，其中包括调用 `propagate` 方法来将邻域结点的信息（information）传播到中央结点。一般来说是预处理 -> 传播 -> 后处理。\n",
        "\n",
        "一般来说，`propagate` 会进行三个步骤：\n",
        "\n",
        "1. 调用 `message` 方法来把邻域结点的信息（information）转变为消息（message）；\n",
        "2. 调用 `aggregate` 来将所有消息从邻域聚合为一；\n",
        "3. 调用 `update` 方法会进一步生成下一次迭代的结点嵌入。\n",
        "\n",
        "但是我们的实现和上述过程稍微有些不一样，因为我们不会实现 `update`，而是将更新结点的逻辑放在 `forward` 方法中。更具体点来说，在信息（information）传播之后，我们直接在 `propagate` 的结果上进行一些操作，使得 `forward` 结果刚好是这次迭代完之后的节点嵌入。\n",
        "\n",
        "除此之外，传递给 `propagate` 函数当参数的张量也可以通过在变量名后加上 `_i` 或 `_j` 后缀来映射为结点 $i$ 和结点 $j$ 相关的张量，比如 $x_j$ 和 $x_j$。注意我们用 $i$ 指代中央节点，用 $j$ 指代邻居结点。\n",
        "\n",
        "注释里会有更多细节。值得注意的是，我们在 **GraphSAGE** 里加入了 **Skip Connection**。更新规则可以用公式更正式地描述为：\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "h_v^{(l)} = W_l\\cdot h_v^{(l-1)} + W_r \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\})\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "简单起见，我们这里使用了平均聚合，其中\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\}) = \\frac{1}{|N(v)|} \\sum_{u\\in N(v)} h_u^{(l-1)}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "除此之外, 每次迭代都会加入 $\\ell$-2 正则。\n",
        "\n",
        "> 为了正确地实现，我们得先理解下各个方法之间如何互动。在`propagate` 中，我们传入任何需要地参数。比如，我们将 `x` 当成参数传入：\n",
        "> \n",
        "> $$\n",
        "> \\dots = propagate(\\dots, x=(x_\\text{central}, x_\\text{neighbor}), \\dots)\n",
        "> $$\n",
        "> \n",
        "> 这里的 $x_\\text{central}$ 和 $x_\\text{neighbor}$ 分别代表来自中央和邻域的结点。如果中央和邻域的结点的表征是同一套表征，则 $x_\\text{central}$ 和 $x_\\text{neighbor}$ 一样。\n",
        "> \n",
        "> 不妨假设 $x_\\text{central}$ 和 $x_\\text{neighbor}$ 的形状都是 $N\\times d$，其中 $N$ 是结点个数，$d$ 是特征维度。\n",
        "> \n",
        "> 那么在 `message` 方法中，我们可以取叫做 `x_i` 和 `x_j` 的参数，则这里的 `x_i` 和 `x_j` 的形状都是 $E\\times d$，而不是 $N\\times d$！`x_i`是通过将所有边中的中央结点拼接起来生成的，类似地，`x_j` 是通过将所有边中的邻居结点拼接起来生成的。\n",
        "> \n",
        "> 让我们看看例子。假设我们有四个结点，所以 $x_\\text{central}$ 和 $x_\\text{neighbor}$ 的形状是 $4\\times d$。我们有两条边 (1, 2) 和 (3, 0)，因此 $x_i$ 是$\\left[x_\\text{central}[1]; x_\\text{central}[3]\\right]$, `x_j` 为$\\left[x_\\text{central}[2]; x_\\text{central}[0]\\right]$。\n",
        "> \n",
        "<font color='red'>接下来的问题请不要参考网上的实现！</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RwG4HqCFCaOD"
      },
      "outputs": [],
      "source": [
        "class GraphSage(MessagePassing):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, normalize = True,\n",
        "                 bias = False, **kwargs):  \n",
        "        super(GraphSage, self).__init__(**kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.lin_l = None\n",
        "        self.lin_r = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the layers needed for the message and update functions below.\n",
        "        # self.lin_l is the linear transformation that you apply to embedding \n",
        "        #            for central node.\n",
        "        # self.lin_r is the linear transformation that you apply to aggregated \n",
        "        #            message from neighbors.\n",
        "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.lin_l.reset_parameters()\n",
        "        self.lin_r.reset_parameters()\n",
        "\n",
        "    def forward(self, x, edge_index, size = None):\n",
        "        \"\"\"\"\"\"\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement message passing, as well as any post-processing (our update rule).\n",
        "        # 1. First call propagate function to conduct the message passing.\n",
        "        #    1.1 See there for more information: \n",
        "        #        https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "        #    1.2 We use the same representations for central (x_central) and \n",
        "        #        neighbor (x_neighbor) nodes, which means you'll pass x=(x, x) \n",
        "        #        to propagate.\n",
        "        # 2. Update our node embedding with skip connection.\n",
        "        # 3. If normalize is set, do L-2 normalization (defined in \n",
        "        #    torch.nn.functional)\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x_j):\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your message function here.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size = None):\n",
        "\n",
        "        out = None\n",
        "\n",
        "        # The axis along which to index number of nodes.\n",
        "        node_dim = self.node_dim\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your aggregate function here.\n",
        "        # See here as how to use torch_scatter.scatter: \n",
        "        # https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjcfF3RACdLD"
      },
      "source": [
        "### 实现 GAT\n",
        "\n",
        "注意力机制现在在许多基于次序的任务中成为了 Sota 模型，比如机器翻译、学习句子表征等任务。注意力机制的一个主要好处就是它能关注输入中最相关的部分来做出决策。在本次实验中，让我们通过使用 `GAT` 来看看如何使用注意力机制进行图结构数据的结点分类任务。\n",
        "\n",
        "GAT 的基础模块就是图注意力层，是聚合函数的一种变体。假设有 $N$ 个结点，每个结点有 $F$ 维特征向量。则每个图注意力层的输入都是结点特征的集合：$\\mathbf{h} = \\left\\{\\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N}\\right\\}$，$\\overrightarrow{h_i} \\in R^F$，输出则是新的结点特征集，但是维度可能变成了 $F^\\prime$：$\\mathbf{h^\\prime} = \\left\\{\\overrightarrow{h_1^\\prime}, \\overrightarrow{h_2^\\prime}, \\dots, \\overrightarrow{h_N^\\prime}\\right\\}$，其中 $\\overrightarrow{h_i^\\prime} \\in \\mathbb{R}^{F^\\prime}$。\n",
        "\n",
        "我们现在来讲讲图注意力层如何将输入特征转化成更高级别的特征。首先，每个结点都需要经过一个公用的线性变换，即参数矩阵$\\mathbf{W} \\in \\mathbb{R}^{F^\\prime \\times F}$。然后，再给每个结点加上自注意力机制：\n",
        "\n",
        "$$\n",
        "\\begin{equation} \n",
        "a : \\mathbb{R}^{F'} \\times \\mathbb{R}^{F'} \\rightarrow \\mathbb{R}.\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "这个机制会计算出注意力系数，而注意力系数则代表结点 $j$ 的特征对于结点 $i$ 的重要程度：\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "e_{ij} = a\\left(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j}\\right)\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "最一般的自注意力机制就是丢掉结构信息，然后每个结点都对其他所有结点计算注意力。但为了使用图结构信息，我们在注意力机制中使用的是掩码注意力（masked attention）。在掩码注意力中，我们只计算邻域结点的注意力系数 $e_{ij}$。\n",
        "\n",
        "为了方便比较不同结点的系数，我们关于 $j$ 对系数使用 softmax 函数进行了归一化：\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "我们的注意力机制 $a$ 是一个单层前播神经网络，其中的参数为$\\overrightarrow{a} \\in \\mathbb{R}^{F^\\prime}$，之后会有一个 LeakyReLU 非线性（负数输入的斜率是0.2）。让我们将 $\\cdot^\\top$ 记为转置，记 $\\|$ 为拼接，则我们注意力机制则可以用公式表述为\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\alpha_{ij} = \\frac{\\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^\\top \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^\\top\\mathbf{W_r}\\overrightarrow{h_j}\\Big)\\Big)}{\\sum_{k\\in \\mathcal{N}_i} \\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^\\top \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^\\top\\mathbf{W_r}\\overrightarrow{h_k}\\Big)\\Big)}\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "在后续问题中，我们约定：$\\alpha_l = [...,\\overrightarrow{a_l}^\\top \\mathbf{W_l} \\overrightarrow{h_i},...]$ 以及 $\\alpha_r = [..., \\overrightarrow{a_r}^\\top \\mathbf{W_r} \\overrightarrow{h_j}, ...]$。\n",
        "\n",
        "在 GAT 中的每一层中，计算完注意力系数之后，聚合函数的结果都可由邻域消息加权求和得到，其中权重都由 $\\alpha_{ij}$ 指定。\n",
        "\n",
        "目前，我们通常使用使用归一化的注意力系数来计算邻域内特征的线性组合。这些聚合结果最终就是输出的结点特征。\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "h_i^\\prime = \\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}.\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "为了稳定自注意力的学习过程，我们采用多头注意力机制。即使用 $K$ 个独立的注意力机制，或者叫“头”，来计算出上式的输出特征。然后我们将这些输出特征拼接起来：\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    \\overrightarrow{h_i}^\\prime = \\|_{k=1}^K \\Big(\\sum_{j \\in \\mathcal{N}_i} \\alpha_{ij}^{(k)} \\mathbf{W_r}^{(k)} \\overrightarrow{h_j}\\Big)\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "其中 $\\|$ 是拼接，$\\alpha_{ij}^{(k)}$ 是 $k$ 头归一化的注意力机制 $(a^k)$，且 $\\mathbf{W}^{(k)}$ 是对应输入线性变换的权重矩阵。注意，在当前情况下，$\\mathbf{h'} \\in \\mathbb{R}^{KF'}$。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w4j45gTpCeXO"
      },
      "outputs": [],
      "source": [
        "class GAT(MessagePassing):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, heads = 2,\n",
        "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
        "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.heads = heads\n",
        "        self.negative_slope = negative_slope\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.lin_l = None\n",
        "        self.lin_r = None\n",
        "        self.att_l = None\n",
        "        self.att_r = None\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the layers needed for the message functions below.\n",
        "        # self.lin_l is the linear transformation that you apply to embeddings \n",
        "        # BEFORE message passing.\n",
        "        # Pay attention to dimensions of the linear layers, since we're using \n",
        "        # multi-head attention.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        self.lin_r = self.lin_l\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Define the attention parameters \\overrightarrow{a_l/r}^T in the above intro.\n",
        "        # You have to deal with multi-head scenarios.\n",
        "        # Use nn.Parameter instead of nn.Linear\n",
        "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
        "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
        "        nn.init.xavier_uniform_(self.att_l)\n",
        "        nn.init.xavier_uniform_(self.att_r)\n",
        "\n",
        "    def forward(self, x, edge_index, size = None):\n",
        "        \n",
        "        H, C = self.heads, self.out_channels\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement message passing, as well as any pre- and post-processing (our update rule).\n",
        "        # 1. First apply linear transformation to node embeddings, and split that \n",
        "        #    into multiple heads. We use the same representations for source and\n",
        "        #    target nodes, but apply different linear weights (W_l and W_r)\n",
        "        # 2. Calculate alpha vectors for central nodes (alpha_l) and neighbor nodes (alpha_r).\n",
        "        # 3. Call propagate function to conduct the message passing. \n",
        "        #    3.1 Remember to pass alpha = (alpha_l, alpha_r) as a parameter.\n",
        "        #    3.2 See there for more information: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
        "        # 4. Transform the output back to the shape of N * d.\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your message function. Putting the attention in message \n",
        "        # instead of in update is a little tricky.\n",
        "        # 1. Calculate the final attention weights using alpha_i and alpha_j,\n",
        "        #    and apply leaky Relu.\n",
        "        # 2. Calculate softmax over the neighbor nodes for all the nodes. Use \n",
        "        #    torch_geometric.utils.softmax instead of the one in Pytorch.\n",
        "        # 3. Apply dropout to attention weights (alpha).\n",
        "        # 4. Multiply embeddings and attention weights. As a sanity check, the output\n",
        "        #    should be of shape E * H * d.\n",
        "        # 5. ptr (LongTensor, optional): If given, computes the softmax based on\n",
        "        #    sorted inputs in CSR representation. You can simply pass it to softmax.\n",
        "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def aggregate(self, inputs, index, dim_size = None):\n",
        "\n",
        "        ############################################################################\n",
        "        # TODO: Your code here! \n",
        "        # Implement your aggregate function here.\n",
        "        # See here as how to use torch_scatter.scatter: https://pytorch-scatter.readthedocs.io/en/latest/_modules/torch_scatter/scatter.html\n",
        "        # Pay attention to \"reduce\" parameter is different from that in GraphSage.\n",
        "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
        "\n",
        "\n",
        "        ############################################################################\n",
        "    \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2dkgSuWCheU"
      },
      "source": [
        "### 构建优化器\n",
        "\n",
        "这个函数已经替你实现了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_TIQ8NPCjBP"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def build_optimizer(args, params):\n",
        "    weight_decay = args.weight_decay\n",
        "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
        "    if args.opt == 'adam':\n",
        "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'sgd':\n",
        "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
        "    elif args.opt == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    elif args.opt == 'adagrad':\n",
        "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
        "    if args.opt_scheduler == 'none':\n",
        "        return None, optimizer\n",
        "    elif args.opt_scheduler == 'step':\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
        "    elif args.opt_scheduler == 'cos':\n",
        "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
        "    return scheduler, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBYdWFwYCkwY"
      },
      "source": [
        "### 训练和测试\n",
        "\n",
        "这里我们给你提供了训练和测试的函数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tZMWRc8CmGg"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def train(dataset, args):\n",
        "    \n",
        "    print(\"Node task. test set size:\", np.sum(dataset[0]['train_mask'].numpy()))\n",
        "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "\n",
        "    # build model\n",
        "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes, \n",
        "                            args)\n",
        "    scheduler, opt = build_optimizer(args, model.parameters())\n",
        "\n",
        "    # train\n",
        "    losses = []\n",
        "    test_accs = []\n",
        "    for epoch in range(args.epochs):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            opt.zero_grad()\n",
        "            pred = model(batch)\n",
        "            label = batch.y\n",
        "            pred = pred[batch.train_mask]\n",
        "            label = label[batch.train_mask]\n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "        total_loss /= len(loader.dataset)\n",
        "        losses.append(total_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "          test_acc = test(test_loader, model)\n",
        "          test_accs.append(test_acc)\n",
        "        else:\n",
        "          test_accs.append(test_accs[-1])\n",
        "    return test_accs, losses\n",
        "\n",
        "def test(loader, model, is_validation=True):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            # max(dim=1) returns values, indices tuple; only need indices\n",
        "            pred = model(data).max(dim=1)[1]\n",
        "            label = data.y\n",
        "\n",
        "        mask = data.val_mask if is_validation else data.test_mask\n",
        "        # node classification: only evaluate on nodes in test set\n",
        "        pred = pred[mask]\n",
        "        label = data.y[mask]\n",
        "            \n",
        "        correct += pred.eq(label).sum().item()\n",
        "\n",
        "    total = 0\n",
        "    for data in loader.dataset:\n",
        "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
        "    return correct / total\n",
        "  \n",
        "class objectview(object):\n",
        "    def __init__(self, **d):\n",
        "        self.__dict__ = d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7-h7jIsCns4"
      },
      "source": [
        "### 开始训练！\n",
        "\n",
        "我们将在 CORA 数据集上进行结点分类任务。这一部分已经替你实现好了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe9B45l9Cpz2"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    args = objectview(\n",
        "        model_type=\"GraphSage\",\n",
        "        dataset=\"cora\",\n",
        "        num_layers=2,\n",
        "        heads=1,\n",
        "        batch_size=32,\n",
        "        hidden_dim=32,\n",
        "        dropout=0.5,\n",
        "        epochs=500,\n",
        "        opt=\"adam\",\n",
        "        opt_scheduler=\"none\",\n",
        "        opt_restart=0,\n",
        "        weight_decay=5e-3,\n",
        "        lr=0.01\n",
        "    )\n",
        "    for model_type in [\"GraphSage\", \"GAT\"]:\n",
        "        args.model_type = model_type\n",
        "        args.heads      = 2 if model_type == \"GAT\" else 1\n",
        "        dataset         = Planetoid(root=\"./dataset/\", name=\"Cora\")\n",
        "\n",
        "        test_accs, losses = train(dataset, args)\n",
        "        print(f\"Maximum accuracy: {max(test_accs)}\")\n",
        "        print(f\"Minimum loss: {min(losses)}\")\n",
        "\n",
        "        plt.title(dataset.name)\n",
        "        plt.plot(losses, label=f\"training loss - {model_type}\")\n",
        "        plt.plot(test_accs, label=f\"test accuracy - {model_type}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHELqjARZ1W5"
      },
      "source": [
        "### 问题 1.1：你能使用GraphSAGE在测试集上获得的最大准确度是多少？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlCtBEBLMBkR"
      },
      "source": [
        "### 问题 1.2：你能使用GAT在测试集上获得的最大准确度是多少？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwwq0nSdmsOL"
      },
      "source": [
        "## 2 DeepSNAP 基础\n",
        "\n",
        "之前的实验中，我们使用了图类（`NetworkX`）和张量（`PyG`）来表征图。图类`nx.Graph`提供了丰富的分析和操作功能，比如聚合系数和PageRank。为了将图喂进模型，我们得把图用张量表征，包括边张量`edge_index`和结点特征`x`及标签`y`。但是只用张量会让图操作和分析变得困难且低效。所以，本实验中我们将使用既能提供表征方式，又能提供GNN训练、验证、测试完整流水线的 DeepSNAP。\n",
        "\n",
        "概括来说，[DeepSNAP](https://github.com/snap-stanford/deepsnap) 是一个 Python 库，可以辅助我们进行高效的图深度学习。DeepSNAP 支持灵活的图操作，标准流水线，以及异构图和简单的API。\n",
        "\n",
        "1. DeepSNAP 可以很容易地在训练前或中进行复杂地图操作，比如特征计算，预训练，子图抽取等。\n",
        "2. 在大多数框架中，归纳式或是直推式的结点、边、以及图任务的标准流水线都得用户来编码。实际上，还涉及到一些额外的选择（比如如何分割用于连接预测的数据集）。DeepSNAP 提供了这样的标准流水线，节省了很多编码的时间，并且让不同模型之间的比较更加公平。\n",
        "3. 许多现实世界的数据集是异构网络，但是支持异构网络（包括数据存储和灵活的消息传递）的库很少。DeepSNAP提供了一套高效灵活、支持结点和边异构的工具。\n",
        "\n",
        "[DeepSNAP](https://github.com/snap-stanford/deepsnap) 是最新发布的项目，且仍在开发。如果你发现了任何 bug 或有任何改进想法，请随时在 GitHub 上提出 issue 或是发起 pull requests。\n",
        "\n",
        "本次实验中，我们将关注于 DeepSNAP 中的图操作和分割功能。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20SvvngpQmmQ"
      },
      "source": [
        "### 配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfbBVFmAQlwz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from torch_geometric.datasets import Planetoid, TUDataset\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def visualize(G, color_map=None, seed=123):\n",
        "  if color_map is None:\n",
        "    color_map = '#c92506'\n",
        "  plt.figure(figsize=(8, 8))\n",
        "  nodes = nx.draw_networkx_nodes(G, pos=nx.spring_layout(G, seed=seed), \\\n",
        "                                 label=None, node_color=color_map, node_shape='o', node_size=150)\n",
        "  edges = nx.draw_networkx_edges(G, pos=nx.spring_layout(G, seed=seed), alpha=0.5)\n",
        "  if color_map is not None:\n",
        "    plt.scatter([],[], c='#c92506', label='Nodes with label 0', edgecolors=\"black\", s=140)\n",
        "    plt.scatter([],[], c='#fcec00', label='Nodes with label 1', edgecolors=\"black\", s=140)\n",
        "    plt.legend(prop={'size': 13}, handletextpad=0)\n",
        "  nodes.set_edgecolor('black')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-o1P3r6hr2"
      },
      "source": [
        "### DeepSNAP 图\n",
        "\n",
        "`deepsnap.graph.Graph` 类是 DeepSNAP 中的核心类。它不仅将图表征为张量，还引用了来自图操作包的图对象。\n",
        "\n",
        "目前 DeepSNAP 支持 [NetworkX](https://networkx.org/) 和 [Snap.py](https://snap.stanford.edu/snappy/doc/index.html) 作为图操作包后端。\n",
        "\n",
        "本实验中，我们将使用 `Networkx` 作为后端图操作包。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ispq_lIoJl_z"
      },
      "source": [
        "首先来试试将一个简单的随机 `NetworkX` 图转化成 `DeepSNAP` 图。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5qca3x6XpG"
      },
      "outputs": [],
      "source": [
        "num_nodes = 100\n",
        "p = 0.05\n",
        "seed = 100\n",
        "\n",
        "# Generate a networkx random graph\n",
        "G = nx.gnp_random_graph(num_nodes, p, seed=seed)\n",
        "\n",
        "# Generate some random node features and labels\n",
        "node_feature = {node : torch.rand([5, ]) for node in G.nodes()}\n",
        "node_label = {node : torch.randint(0, 2, ()) for node in G.nodes()}\n",
        "\n",
        "# Set the random features and labels to G\n",
        "nx.set_node_attributes(G, node_feature, name='node_feature')\n",
        "nx.set_node_attributes(G, node_label, name='node_label')\n",
        "\n",
        "# Print one node example\n",
        "for node in G.nodes(data=True):\n",
        "  print(node)\n",
        "  break\n",
        "\n",
        "color_map = ['#c92506' if node[1].item() == 0 else '#fcec00' for node in G.nodes(data=\"node_label\")]\n",
        "\n",
        "# Visualize the graph\n",
        "visualize(G, color_map=color_map)\n",
        "\n",
        "# Transform the networkx graph into the deepsnap graph\n",
        "graph = Graph(G)\n",
        "\n",
        "# Print out the general deepsnap graph information\n",
        "print(graph)\n",
        "\n",
        "# DeepSNAP will convert node attributes to tensors\n",
        "# Notice the type of tensors\n",
        "print(f\"Node feature (node_feature) has shape {graph.node_feature.shape} and type {graph.node_feature.dtype}\")\n",
        "print(f\"Node label (node_label) has shape {graph.node_label.shape} and type {graph.node_label.dtype}\")\n",
        "\n",
        "# DeepSNAP will also generate the edge_index tensor\n",
        "print(f\"Edge index (edge_index) has shape {graph.edge_index.shape} and type {graph.edge_index.dtype}\")\n",
        "\n",
        "# Different from only storing tensors, deepsnap graph also references to the networkx graph\n",
        "# We will discuss why the reference will be helpful later\n",
        "print(f\"The DeepSNAP graph has {type(graph.G)} as the internal manupulation graph\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNMbc307KOQD"
      },
      "source": [
        "DeepSNAP 有三级属性。在本例中有**结点级**属性，比如结点特征 `node_feature` 和结点标签 `node_label`。其他两级分别是**图级**和**边级**属性。用法和**结点级**类似，除了特征变成`edge_feature`或`graph_feature`，标签变成`edge_label`或`graph_label`等。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Xz58_Da0qL"
      },
      "source": [
        "与 NetworkX 图类似，我们可以通过对象属性直接获取图的基本信息。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLo4zWAoeg6S"
      },
      "outputs": [],
      "source": [
        "# Number of nodes\n",
        "print(f\"The random graph has {graph.num_nodes} nodes\")\n",
        "\n",
        "# Number of edges\n",
        "print(f\"The random graph has {graph.num_edges} edges\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po7IaRmwblI5"
      },
      "source": [
        "DeepSNAP 也提供了自动将 `PyG` 数据集对象转化成一个包含 `DeepSNAP` 图对象列表的函数。\n",
        "\n",
        "这里我们将 `Cora` 数据集转化成 `DeepSNAP` 图的列表试试。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFkg2kCgcFwR"
      },
      "outputs": [],
      "source": [
        "root = './dataset'\n",
        "name = 'Cora'\n",
        "\n",
        "# The Cora dataset\n",
        "pyg_dataset= Planetoid(root, name)\n",
        "\n",
        "# PyG dataset to a list of deepsnap graphs\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Get the first deepsnap graph (CORA only has one graph)\n",
        "graph = graphs[0]\n",
        "print(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLm5vVYMAP2x"
      },
      "source": [
        "### 问题2.1：CORA 图中有多少分类？特征维度又是多少？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iF_Kyqr_JbY"
      },
      "outputs": [],
      "source": [
        "def get_num_node_classes(graph):\n",
        "  # TODO: Implement this function that takes a deepsnap graph object\n",
        "  # and return the number of node classes of that graph.\n",
        "\n",
        "  num_node_classes = 0\n",
        "\n",
        "  ############# Your code here #############\n",
        "  ## (~1 line of code)\n",
        "  ## Note\n",
        "  ## 1. Colab autocomplete functionality might be useful\n",
        "  ## 2. DeepSNAP documentation might be useful https://snap.stanford.edu/deepsnap/modules/graph.html\n",
        "\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "  return num_node_classes\n",
        "\n",
        "def get_num_node_features(graph):\n",
        "  # TODO: Implement this function that takes a deepsnap graph object\n",
        "  # and return the number of node features of that graph.\n",
        "\n",
        "  num_node_features = 0\n",
        "\n",
        "  ############# Your code here #############\n",
        "  ## (~1 line of code)\n",
        "  ## Note\n",
        "  ## 1. Colab autocomplete functionality might be useful\n",
        "  ## 2. DeepSNAP documentation might be useful https://snap.stanford.edu/deepsnap/modules/graph.html\n",
        "\n",
        "\n",
        "  ##########################################\n",
        "\n",
        "  return num_node_features\n",
        "\n",
        "num_node_classes = get_num_node_classes(graph)\n",
        "num_node_features = get_num_node_features(graph)\n",
        "print(f\"{name} has {num_node_classes} classes\")\n",
        "print(f\"{name} has {num_node_features} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwKbzhHUAckZ"
      },
      "source": [
        "### DeepSNAP 数据集\n",
        "\n",
        "现在，让我们来看看 DeepSNAP 数据集。一个 `deepsnap.dataset.GraphDataset` 会包含一列 `deepsnap.graph.Graph` 对象。除了 `deepsnap.graph.Graph` 对象之外，你也可以指定这个数据集会用来进行什么任务，比如**结点级**的任务（`task=node`），**边级**任务（`task=link_pred`）以及**图级**任务（`task=graph`）。\n",
        "\n",
        "`deepsnap.dataset.GraphDataset` 也包含初始化时许多其他有用的参数以及其他功能。如果你很感兴趣，可以去看看[文档](https://snap.stanford.edu/deepsnap/modules/dataset.html#deepsnap-graphdataset)。\n",
        "\n",
        "现在先使用有很多图的 COX2 数据集，并在初始化的时候指定任务为 `graph`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4kqUldyoaS_"
      },
      "outputs": [],
      "source": [
        "root = './dataset'\n",
        "name = 'COX2'\n",
        "\n",
        "# Load the dataset through PyG\n",
        "pyg_dataset = TUDataset(root, name)\n",
        "\n",
        "# Convert to a list of deepsnap graphs\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Convert list of deepsnap graphs to deepsnap dataset with specified task=graph\n",
        "dataset = GraphDataset(graphs, task='graph')\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCV3xJWCddX"
      },
      "source": [
        "### 问题2.2：在COX2数据集索引为100的图中，标签是什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIis9oTZAfs3"
      },
      "outputs": [],
      "source": [
        "def get_graph_class(dataset, idx):\n",
        "  # TODO: Implement this function that takes a deepsnap dataset object,\n",
        "  # the index of the graph in the dataset, and returns the class/label \n",
        "  # of the graph (in integer).\n",
        "\n",
        "  label = -1\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  ## Note\n",
        "  ## 1. The label refers to the graph-level attribute\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return label\n",
        "\n",
        "graph_0 = dataset[0]\n",
        "print(graph_0)\n",
        "idx = 100\n",
        "label = get_graph_class(dataset, idx)\n",
        "print(f'Graph with index {idx} has label {label}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhcVeAhCwoY"
      },
      "source": [
        "### 问题2.3：在COX2数据集索引为200的图中，一共有多少边？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5m2DOfhBtWv"
      },
      "outputs": [],
      "source": [
        "def get_graph_num_edges(dataset, idx):\n",
        "  # TODO: Implement this function that takes a deepsnap dataset object,\n",
        "  # the index of the graph in dataset, and returns the number of \n",
        "  # edges in the graph (in integer).\n",
        "\n",
        "  num_edges = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 lines of code)\n",
        "  ## Note\n",
        "  ## 1. You can use the class property directly\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return num_edges\n",
        "\n",
        "idx = 200\n",
        "num_edges = get_graph_num_edges(dataset, idx)\n",
        "print(f'Graph with index {idx} has {num_edges} edges')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXa7yIG4E0Fp"
      },
      "source": [
        "## 3 DeepSNAP 进阶\n",
        "\n",
        "我们已经学习了 DeepSNAP 的图和数据集的基本用法！\n",
        "\n",
        "现在继续来学习一些高阶功能。\n",
        "\n",
        "本小节我们会使用 DeepSNAP 进行特征计算和归纳式/直推式分割数据集。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5fsGBLY8cxa"
      },
      "source": [
        "### 配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-jgRLiQ8cSj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from torch_geometric.datasets import Planetoid, TUDataset\n",
        "\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnazPGGAJAZN"
      },
      "source": [
        "### 图的数据分割\n",
        "\n",
        "图的数据分割可比 CV 和 NLP 难得多。总体来说，图的数据分割有两种，**归纳式**和**直推式**。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9KG_MhqsWBp"
      },
      "source": [
        "### 归纳式分割\n",
        "\n",
        "正如 Lecture 中讲得，归纳式分割会将多个图分配到训练集、验证集以及测试集中。\n",
        "\n",
        "下面是一个例子，是 DeepSNAP 在**图级任务**（图分类等）中进行归纳式分割。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpc6bTm3GF02"
      },
      "outputs": [],
      "source": [
        "root = './dataset'\n",
        "name = 'COX2'\n",
        "\n",
        "pyg_dataset = TUDataset(root, name)\n",
        "\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Here we specify the task as graph-level task such as graph classification\n",
        "task = 'graph'\n",
        "dataset = GraphDataset(graphs, task=task)\n",
        "\n",
        "# Specify transductive=False (inductive)\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=False, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "print(f\"COX2 train dataset: {dataset_train}\")\n",
        "print(f\"COX2 validation dataset: {dataset_val}\")\n",
        "print(f\"COX2 test dataset: {dataset_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWKQwa4WsgQp"
      },
      "source": [
        "### 直推式分割\n",
        "\n",
        "在直推式分割中，训练集、验证集、测试集都是在一张图上。\n",
        "\n",
        "这里我们在**结点级**任务中直推式地将 CORA 图分割。\n",
        "\n",
        "> 注意在 DeepSNAP 中，默认是随机分割，但你也可以通过在从 `PyG` 中加载数据集时，指定 `fixed_split=True` 来进行固定分割，也可以直接改变 `node_label_index`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5OdxSg4sfyR"
      },
      "outputs": [],
      "source": [
        "root = './dataset'\n",
        "name = 'Cora'\n",
        "\n",
        "pyg_dataset = Planetoid(root, name)\n",
        "\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Here we specify the task as node-level task such as node classification\n",
        "task = 'node'\n",
        "\n",
        "dataset = GraphDataset(graphs, task=task)\n",
        "\n",
        "# Specify we want the transductive splitting\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "print(f\"Cora train dataset: {dataset_train}\")\n",
        "print(f\"Cora validation dataset: {dataset_val}\")\n",
        "print(f\"Cora test dataset: {dataset_test}\")\n",
        "\n",
        "print(f\"Original Cora has {dataset.num_nodes[0]} nodes\")\n",
        "\n",
        "# The nodes in each set can be find in node_label_index\n",
        "print(f\"After the split, Cora has {dataset_train[0].node_label_index.shape[0]} training nodes\")\n",
        "print(f\"After the split, Cora has {dataset_val[0].node_label_index.shape[0]} validation nodes\")\n",
        "print(f\"After the split, Cora has {dataset_test[0].node_label_index.shape[0]} test nodes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ePKgM00lGE"
      },
      "source": [
        "### 边级分割\n",
        "\n",
        "与**结点级**和**图级**分割相比，**边级**分割有些麻烦。\n",
        "\n",
        "通常在**边级**分割中，我们需要采样一些负边，将正边分配到不同数据集中，把训练边分为消息传递边和监督边，然后在训练的时候重新采样负边。\n",
        "\n",
        "本小节将介绍`all`模式和`disjoint`模式，如果你对更多图分割的模式感兴趣，可以参考 DeepSNAP 数据集[文档](https://snap.stanford.edu/deepsnap/modules/dataset.html)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnzISX5RoiR6"
      },
      "source": [
        "#### `all` 模式\n",
        "\n",
        "现在让我们从一个简单的**边级**分割模式开始，即 DeepSNAP 中的 `edge_train_mode=\"all\"`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_D104xO6137n"
      },
      "outputs": [],
      "source": [
        "root = './dataset'\n",
        "name = 'Cora'\n",
        "\n",
        "pyg_dataset = Planetoid(root, name)\n",
        "\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "# Specify task as link_pred for edge-level task\n",
        "task = 'link_pred'\n",
        "\n",
        "# Specify the train mode, \"all\" mode is default for deepsnap dataset\n",
        "edge_train_mode = \"all\"\n",
        "\n",
        "dataset = GraphDataset(graphs, task=task, edge_train_mode=edge_train_mode)\n",
        "\n",
        "# Transductive link prediction split\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "print(f\"Cora train dataset: {dataset_train}\")\n",
        "print(f\"Cora validation dataset: {dataset_val}\")\n",
        "print(f\"Cora test dataset: {dataset_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GscopwOXC_Y7"
      },
      "source": [
        "在 DeepSNAP 中，监督边的索引都存储在 `edge_label_index` 张量中，对应的标签存放在 `edge_label` 张量中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJF8fZnA2eLR"
      },
      "outputs": [],
      "source": [
        "print(f\"Original Cora graph has {dataset[0].num_edges} edges\")\n",
        "print(f\"Because Cora graph is undirected, the original edge_index has shape {dataset[0].edge_index.shape}\")\n",
        "\n",
        "print(f\"The training set has message passing edge index shape {dataset_train[0].edge_index.shape}\")\n",
        "print(f\"The training set has supervision edge index shape {dataset_train[0].edge_label_index.shape}\")\n",
        "\n",
        "print(f\"The validation set has message passing edge index shape {dataset_val[0].edge_index.shape}\")\n",
        "print(f\"The validation set has supervision edge index shape {dataset_val[0].edge_label_index.shape}\")\n",
        "\n",
        "print(f\"The test set has message passing edge index shape {dataset_test[0].edge_index.shape}\")\n",
        "print(f\"The test set has supervision edge index shape {dataset_test[0].edge_label_index.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6BX-I_oEKQX"
      },
      "source": [
        "我们可以看到在 `all` 模式下，训练集和验证集的消息传递边 `edge_index` 完全一样。除此之外，在训练集中，正监督边（`edge_label_index`）也和消息传递边一样。但是，在测试集中，消息传递边是训练集和验证集的并集。\n",
        "\n",
        "注意 `edge_label` 和 `edge_label_index` 都有负边（默认情况下，有多少正边就有多少负边）。\n",
        "\n",
        "现在，让我们实现个函数来看看两个存储着边索引的张量是否不相交，再看看更多的边分割属性。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOZHDskbAKN6"
      },
      "source": [
        "#### 问题3.1-3.5：实现检查两个`edge_index`张量不相交的函数，然后回答后续的判断题。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgRYdyPp8EmO"
      },
      "outputs": [],
      "source": [
        "def edge_indices_disjoint(edge_index_1, edge_index_2):\n",
        "  # TODO: Implement this function that takes two edge index tensors,\n",
        "  # and returns whether these two edge index tensors are disjoint.\n",
        "  disjoint = None\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~5 lines of code)\n",
        "  ## Note\n",
        "  ## 1. Here disjoint means that there is no single edge belongs to either edge index tensors\n",
        "  ## 2. You do not need to consider the undirected case. For example, if edge_index_1 contains\n",
        "  ## edge (a, b) and edge_index_2 contains edge (b, a). We will treat them as disjoint in this\n",
        "  ## function.\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "  return disjoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL4ASIDDEIUf"
      },
      "outputs": [],
      "source": [
        "num_train_edges = dataset_train[0].edge_label_index.shape[1] // 2\n",
        "train_pos_edge_index = dataset_train[0].edge_label_index[:, :num_train_edges]\n",
        "train_neg_edge_index = dataset_train[0].edge_label_index[:, num_train_edges:]\n",
        "print(f\"3.1 Training (supervision) positve and negative edges are disjoint = \\\n",
        "        {edge_indices_disjoint(train_pos_edge_index, train_neg_edge_index)}\")\n",
        "\n",
        "num_val_edges = dataset_val[0].edge_label_index.shape[1] // 2\n",
        "val_pos_edge_index = dataset_val[0].edge_label_index[:, :num_val_edges]\n",
        "val_neg_edge_index = dataset_val[0].edge_label_index[:, num_val_edges:]\n",
        "print(f\"3.2 Validation (supervision) positve and negative edges are disjoint = \\\n",
        "        {edge_indices_disjoint(val_pos_edge_index, val_neg_edge_index)}\")\n",
        "\n",
        "num_test_edges = dataset_test[0].edge_label_index.shape[1] // 2\n",
        "test_pos_edge_index = dataset_test[0].edge_label_index[:, :num_test_edges]\n",
        "test_neg_edge_index = dataset_test[0].edge_label_index[:, num_test_edges:]\n",
        "print(f\"3.3 Test (supervision) positve and negative edges are disjoint = \\\n",
        "        {edge_indices_disjoint(test_pos_edge_index, test_neg_edge_index)}\")\n",
        "\n",
        "print(f\"3.4 Test (supervision) positve and validation (supervision) positve edges are disjoint = \\\n",
        "        {edge_indices_disjoint(test_pos_edge_index, val_pos_edge_index)}\")\n",
        "\n",
        "print(f\"3.5 Validation (supervision) positve and training (supervision) positve edges are disjoint = \\\n",
        "        {edge_indices_disjoint(val_pos_edge_index, train_pos_edge_index)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jLoVN5ZBTuA"
      },
      "source": [
        "#### `Disjoint` 模式\n",
        "\n",
        "现在来看看一个相对来说更复杂的直推式边分割案例，即 DeepSNAP 中 `edge_train_mode=\"disjoint\"` 的模式，也是 Lecture 中讲的直推式链接预测分割。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rqzfb-0BTBm"
      },
      "outputs": [],
      "source": [
        "edge_train_mode = \"disjoint\"\n",
        "\n",
        "dataset = GraphDataset(graphs, task='link_pred', edge_train_mode=edge_train_mode)\n",
        "orig_edge_index = dataset[0].edge_index\n",
        "dataset_train, dataset_val, dataset_test = dataset.split(\n",
        "    transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "\n",
        "train_message_edge_index = dataset_train[0].edge_index\n",
        "train_sup_edge_index = dataset_train[0].edge_label_index\n",
        "val_sup_edge_index = dataset_val[0].edge_label_index\n",
        "test_sup_edge_index = dataset_test[0].edge_label_index\n",
        "\n",
        "print(f\"The edge index of original graph has shape: {orig_edge_index.shape}\")\n",
        "print(f\"The edge index of training message edges has shape: {train_message_edge_index.shape}\")\n",
        "print(f\"The edge index of training supervision edges has shape: {train_sup_edge_index.shape}\")\n",
        "print(f\"The edge index of validation message edges has shape: {dataset_val[0].edge_index.shape}\")\n",
        "print(f\"The edge index of validation supervision edges has shape: {val_sup_edge_index.shape}\")\n",
        "print(f\"The edge index of test message edges has shape: {dataset_test[0].edge_index.shape}\")\n",
        "print(f\"The edge index of test supervision edges has shape: {test_sup_edge_index.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUkBhiJNciol"
      },
      "source": [
        "可以看到，在两种模式下，训练集与验证集的消息传递边以及训练监督边的分割情况是不一样的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WKfRjqAJHtK"
      },
      "source": [
        "#### 重新负采样\n",
        "\n",
        "在训练阶段，每迭代一次，一般就要重新负采样一次。\n",
        "\n",
        "下面我们把两次训练迭代过程中的训练集和验证集中的负边打印出来。\n",
        "\n",
        "你应该能发现训练集中的负边被重新采样了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMEbnx63JHWj"
      },
      "outputs": [],
      "source": [
        "dataset = GraphDataset(graphs, task='link_pred', edge_train_mode=\"disjoint\")\n",
        "datasets = {}\n",
        "follow_batch = []\n",
        "datasets['train'], datasets['val'], datasets['test'] = dataset.split(\n",
        "    transductive=True, split_ratio=[0.8, 0.1, 0.1])\n",
        "dataloaders = {\n",
        "  split: DataLoader(\n",
        "    ds, collate_fn=Batch.collate(follow_batch),\n",
        "    batch_size=1, shuffle=(split=='train')\n",
        "  )\n",
        "  for split, ds in datasets.items()\n",
        "}\n",
        "neg_edges_1 = None\n",
        "for batch in dataloaders['train']:\n",
        "  num_edges = batch.edge_label_index.shape[1] // 2\n",
        "  neg_edges_1 = batch.edge_label_index[:, num_edges:]\n",
        "  print(f\"First iteration training negative edges:\\t{neg_edges_1}\")\n",
        "  break\n",
        "neg_edges_2 = None\n",
        "for batch in dataloaders['train']:\n",
        "  num_edges = batch.edge_label_index.shape[1] // 2\n",
        "  neg_edges_2 = batch.edge_label_index[:, num_edges:]\n",
        "  print(f\"Second iteration training negative edges:\\t{neg_edges_2}\")\n",
        "  break\n",
        "\n",
        "neg_edges_1 = None\n",
        "for batch in dataloaders['val']:\n",
        "  num_edges = batch.edge_label_index.shape[1] // 2\n",
        "  neg_edges_1 = batch.edge_label_index[:, num_edges:]\n",
        "  print(f\"First iteration validation negative edges:\\t{neg_edges_1}\")\n",
        "  break\n",
        "neg_edges_2 = None\n",
        "for batch in dataloaders['val']:\n",
        "  num_edges = batch.edge_label_index.shape[1] // 2\n",
        "  neg_edges_2 = batch.edge_label_index[:, num_edges:]\n",
        "  print(f\"Second iteration validation negative edges:\\t{neg_edges_2}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkrYyeSUI_9_"
      },
      "source": [
        "### 图转化 / 特征计算\n",
        "\n",
        "DeepSNAP 另一个核心功能就是图转化 / 特征计算。\n",
        "\n",
        "在 DeepSNAP中，我们将图转化 / 特征计算分成两种类型。一种是在训练前对整个数据集（`deepsnap.dataset`）进行转化，另一种是在训练时分批（`deepsnap.batch`）转化。\n",
        "\n",
        "下面就是一个使用 NetworkX 作为后端来计算 PageRank 值并在训练前对张量进行更新的例子，即训练前对整个数据集转化。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnAVbZINLZ4I"
      },
      "outputs": [],
      "source": [
        "def pagerank_transform_fn(graph):\n",
        "\n",
        "  # Get the referenced networkx graph\n",
        "  G = graph.G\n",
        "\n",
        "  # Calculate the pagerank by using networkx\n",
        "  pr = nx.pagerank(G)\n",
        "\n",
        "  # Transform the pagerank values to tensor\n",
        "  pr_feature = torch.tensor([pr[node] for node in range(graph.num_nodes)], dtype=torch.float32)\n",
        "  pr_feature = pr_feature.view(graph.num_nodes, 1)\n",
        "\n",
        "  # Concat the pagerank values to the node feature\n",
        "  graph.node_feature = torch.cat([graph.node_feature, pr_feature], dim=-1)\n",
        "\n",
        "root = './dataset'\n",
        "name = 'COX2'\n",
        "pyg_dataset = TUDataset(root, name)\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "dataset = GraphDataset(graphs, task='graph')\n",
        "print(f\"Number of features before transformation: {dataset.num_node_features}\")\n",
        "dataset.apply_transform(pagerank_transform_fn, update_tensor=False)\n",
        "print(f\"Number of features after transformation: {dataset.num_node_features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHByE87SQkUw"
      },
      "source": [
        "### 问题3.6：实现下面的转化并报告在COX2数据集图中（索引为406），结点（索引为3）的聚合系数是多少，小数点后取两位。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNEjfOZRNjYb"
      },
      "outputs": [],
      "source": [
        "def cluster_transform_fn(graph):\n",
        "  # TODO: Implement this function that takes an deepsnap graph object,\n",
        "  # transform the graph by adding nodes clustering coefficient into the \n",
        "  # graph.node_feature\n",
        "  pass\n",
        "  ############# Your code here ############\n",
        "  ## (~5 lines of code)\n",
        "  ## Note\n",
        "  ## 1. Compute the clustering coefficient value for each node and\n",
        "  ## concat them to the last dimension of graph.node_feature\n",
        "\n",
        "\n",
        "  #########################################\n",
        "\n",
        "root = './dataset'\n",
        "name = 'COX2'\n",
        "pyg_dataset = TUDataset(root, name)\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "dataset = GraphDataset(graphs, task='graph')\n",
        "\n",
        "# Transform the dataset\n",
        "dataset.apply_transform(cluster_transform_fn, update_tensor=False)\n",
        "\n",
        "node_idx = 3\n",
        "graph_idx = 406\n",
        "node_feature = dataset[graph_idx].node_feature\n",
        "\n",
        "print(f\"The node has clustering coefficient: {round(node_feature[node_idx][-1].item(), 2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P5Ig7XaPYzp"
      },
      "source": [
        "除了能变化整个数据集，DeepSNAP 还可以在训练的时候进行批量（一般指`deepsnap.batch.Batch`）变化。\n",
        "\n",
        "除此之外，DeepSNAP 还支持引用的图和其张量表征之间的变化同步。举个例子，你可以只在变化函数中修改 `NetworkX` 中的图对象，然后通过指定 `update_tensor=True`，该图对应的张量表征也会自动被修改。\n",
        "\n",
        "更多信息请参考 DeepSNAP [文档](https://snap.stanford.edu/deepsnap/)。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-YLYMLFQYqp"
      },
      "source": [
        "## 4 边级预测\n",
        "\n",
        "在上一节中，我们知道了 DeepSNAP 如何在链接预测的任务中直推式地分割边。\n",
        "\n",
        "现在让我们使用 DeepSNAP 和 PyG 来实现**边级**预测，即链接预测模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrKCNtvERypQ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from deepsnap.graph import Graph\n",
        "from deepsnap.batch import Batch\n",
        "from deepsnap.dataset import GraphDataset\n",
        "from torch_geometric.datasets import Planetoid, TUDataset\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class LinkPredModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.2):\n",
        "        super(LinkPredModel, self).__init__()\n",
        "\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, num_classes)\n",
        "\n",
        "        self.loss_fn = None\n",
        "\n",
        "        ############# Your code here #############\n",
        "        ## (~1 line of code)\n",
        "        ## Note\n",
        "        ## 1. Initialize the loss function to BCEWithLogitsLoss\n",
        "\n",
        "\n",
        "        ##########################################\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.conv2.reset_parameters()\n",
        "\n",
        "    def forward(self, batch):\n",
        "        node_feature, edge_index, edge_label_index = batch.node_feature, batch.edge_index, batch.edge_label_index\n",
        "        \n",
        "        ############# Your code here #############\n",
        "        ## (~6 line of code)\n",
        "        ## Note\n",
        "        ## 1. Feed the node feature into the first conv layer\n",
        "        ## 2. Add a ReLU after the first conv layer\n",
        "        ## 3. Add dropout after the ReLU (with probability self.dropout)\n",
        "        ## 4. Feed the output to the second conv layer\n",
        "        ## 5. Select the embeddings of the source nodes and destination nodes\n",
        "        ## by using the edge_label_index and compute the similarity of each pair\n",
        "        ## by dot product\n",
        "\n",
        "        \n",
        "        ##########################################\n",
        "\n",
        "        return pred\n",
        "    \n",
        "    def loss(self, pred, link_label):\n",
        "        return self.loss_fn(pred, link_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuKbGFOu1Ka8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import *\n",
        "\n",
        "def train(model, dataloaders, optimizer, args):\n",
        "    val_max = 0\n",
        "    best_model = model\n",
        "\n",
        "    for epoch in range(1, args[\"epochs\"]):\n",
        "        for i, batch in enumerate(dataloaders['train']):\n",
        "            \n",
        "            batch.to(args[\"device\"])\n",
        "\n",
        "            ############# Your code here #############\n",
        "            ## (~6 lines of code)\n",
        "            ## Note\n",
        "            ## 1. Zero grad the optimizer\n",
        "            ## 2. Compute loss and backpropagate\n",
        "            ## 3. Update the model parameters\n",
        "\n",
        "\n",
        "            ##########################################\n",
        "\n",
        "            log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}, Loss: {}'\n",
        "            score_train = test(model, dataloaders['train'], args)\n",
        "            score_val = test(model, dataloaders['val'], args)\n",
        "            score_test = test(model, dataloaders['test'], args)\n",
        "\n",
        "            print(log.format(epoch, score_train, score_val, score_test, loss.item()))\n",
        "            if val_max < score_val:\n",
        "                val_max = score_val\n",
        "                best_model = copy.deepcopy(model)\n",
        "    return best_model\n",
        "\n",
        "def test(model, dataloader, args):\n",
        "    model.eval()\n",
        "\n",
        "    score = 0\n",
        "\n",
        "    ############# Your code here #############\n",
        "    ## (~5 lines of code)\n",
        "    ## Note\n",
        "    ## 1. Loop through batches in the dataloader\n",
        "    ## 2. Feed the batch to the model\n",
        "    ## 3. Feed the model output to sigmoid\n",
        "    ## 4. Compute the ROC-AUC score by using sklearn roc_auc_score function\n",
        "    ## 5. Edge labels are stored in batch.edge_label\n",
        "\n",
        "    \n",
        "    ##########################################\n",
        " \n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTKWYX1b33V3"
      },
      "outputs": [],
      "source": [
        "# Please don't change any parameters\n",
        "args = {\n",
        "    \"device\" : 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    \"hidden_dim\" : 128,\n",
        "    \"epochs\" : 200,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Klw_xYnE27xQ"
      },
      "outputs": [],
      "source": [
        "pyg_dataset = Planetoid('./dataset', 'Cora')\n",
        "graphs = GraphDataset.pyg_to_graphs(pyg_dataset)\n",
        "\n",
        "dataset = GraphDataset(\n",
        "        graphs,\n",
        "        task='link_pred',\n",
        "        edge_train_mode=\"disjoint\"\n",
        "    )\n",
        "datasets = {}\n",
        "datasets['train'], datasets['val'], datasets['test']= dataset.split(\n",
        "            transductive=True, split_ratio=[0.85, 0.05, 0.1])\n",
        "input_dim = datasets['train'].num_node_features\n",
        "num_classes = datasets['train'].num_edge_labels\n",
        "\n",
        "model = LinkPredModel(input_dim, args[\"hidden_dim\"], num_classes).to(args[\"device\"])\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "dataloaders = {split: DataLoader(\n",
        "            ds, collate_fn=Batch.collate([]),\n",
        "            batch_size=1, shuffle=(split=='train'))\n",
        "            for split, ds in datasets.items()}\n",
        "best_model = train(model, dataloaders, optimizer, args)\n",
        "best_train_roc = test(best_model, dataloaders['train'], args)\n",
        "best_val_roc = test(best_model, dataloaders['val'], args)\n",
        "best_test_roc = test(best_model, dataloaders['test'], args)\n",
        "print(f\"Train: {best_train_roc:.4f}, Val: {best_val_roc:.4f}, Test: {best_test_roc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5brlsKElP0_"
      },
      "source": [
        "### 问题4：你最好的模型 `best_model` 在测试集上能获取到的 ROC-AUC 最高分是多少？"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "CS224W - Colab 3.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('cs224w')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3d401f214986802b882e8a583f41ee2435f6c7e7677ce5b0132581f351a1fd17"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
