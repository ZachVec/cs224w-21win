{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# CS224W - Colab 2\n",
        "\n",
        "本次实验中，我们将使用`PyG`来构建图神经网络，并在两个公开的图测试数据集上测试。这两个数据集可以用来测试在不同的图相关任务上测试模型性能。任务有二，其一为结点分类，其二为图分类。\n",
        "\n",
        "首先，我们要学习一下`PyG`如何将图存储到 PyTorch 张量里。\n",
        "\n",
        "然后，我们将简单的看一下来自`ogb`库中一个数据集。`ogb`包含有很多真实、大规模且多样的图机器学习评测数据集。`ogb`库不仅提供了数据集的加载器，也提供了评估器。\n",
        "\n",
        "最终，我们要用`PyG`来造一个我们自己的图神经网络，并在之前提到的两个数据集上进行测试。\n",
        "\n",
        "**注意**：确保顺序执行所有代码单元，否则变量或库无法在之后的单元中使用。实验愉快 ：）\n",
        "\n",
        "> 在实验开始之前，请确保正确安装了 [PyG](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html) 以及 [obg](https://ogb.stanford.edu/docs/home/) 第三方库。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwwq0nSdmsOL"
      },
      "source": [
        "## 1 PyTorch Geometric (Datasets 和 Data) 教程\n",
        "\n",
        "概括来说，PyTorch Geometric 有两个类可以用来将图存储或变化为张量。其一是使用 `torch_geometric.datasets`，其中有一些常见的图数据集。其二是使用 `torch_geometric.data`，它可以以 `PyTorch` 张量的形式来处理数据。\n",
        "\n",
        "本小节中，我们将学习上述两个类如何使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-o1P3r6hr2"
      },
      "source": [
        "### PyG Datasets\n",
        "\n",
        "`torch_geometric.datasets`中有许多常见图数据集。这里我们将通过一个例子来探索它的用法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT5qca3x6XpG"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "root = './dataset'\n",
        "name = 'ENZYMES'\n",
        "\n",
        "# The ENZYMES dataset\n",
        "pyg_dataset= TUDataset(root, name)\n",
        "\n",
        "# You can find that there are 600 graphs in this dataset\n",
        "print(pyg_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLm5vVYMAP2x"
      },
      "source": [
        "### 问题1：ENZYMES 数据集中有多少类？结点有多少特征？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iF_Kyqr_JbY"
      },
      "outputs": [],
      "source": [
        "def get_num_classes(pyg_dataset):\n",
        "  # TODO: Implement this function that takes a PyG dataset object\n",
        "  # and return the number of classes for that dataset.\n",
        "\n",
        "  num_classes = pyg_dataset.num_classes\n",
        "  return num_classes\n",
        "\n",
        "def get_num_features(pyg_dataset):\n",
        "  # TODO: Implement this function that takes a PyG dataset object\n",
        "  # and return the number of features for that dataset.\n",
        "\n",
        "  num_features = pyg_dataset.num_features\n",
        "  return num_features\n",
        "\n",
        "# You may find that some information need to be stored in the dataset level,\n",
        "# specifically if there are multiple graphs in the dataset\n",
        "\n",
        "num_classes = get_num_classes(pyg_dataset)\n",
        "num_features = get_num_features(pyg_dataset)\n",
        "print(f\"{name} dataset has {num_classes} classes\")\n",
        "print(f\"{name} dataset has {num_features} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwKbzhHUAckZ"
      },
      "source": [
        "### PyG Data\n",
        "\n",
        "每个 `torch_geometric.data.Datasets` 对象通常会存储一个 `torch_geometric.data.Data` 的列表。每个 `torch_geometric.data.Data` 对象一般代表一张图。你通过对 `Dataset`对象索引轻松拿到 `Data` 对象。\n",
        "\n",
        "更多信息可以参考[文档](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)，如 `Data` 对象中到底存储了什么等。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCV3xJWCddX"
      },
      "source": [
        "### 问题2：ENZYMES 数据集中索引为100的图的标签是什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIis9oTZAfs3"
      },
      "outputs": [],
      "source": [
        "def get_graph_class(pyg_dataset, idx):\n",
        "  # TODO: Implement this function that takes a PyG dataset object,\n",
        "  # the index of the graph in dataset, and returns the class/label \n",
        "  # of the graph (in integer).\n",
        "\n",
        "  label = pyg_dataset[idx].y.item()\n",
        "  return label\n",
        "\n",
        "# Here pyg_dataset is a dataset for graph classification\n",
        "graph_0 = pyg_dataset[0]\n",
        "print(graph_0)\n",
        "idx = 100\n",
        "label = get_graph_class(pyg_dataset, idx)\n",
        "print(f\"Graph with index {idx} has label {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhcVeAhCwoY"
      },
      "source": [
        "### 问题3：ENZYMES 数据集中索引为200的图中有多少边？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5m2DOfhBtWv"
      },
      "outputs": [],
      "source": [
        "def get_graph_num_edges(pyg_dataset, idx):\n",
        "  # TODO: Implement this function that takes a PyG dataset object,\n",
        "  # the index of the graph in dataset, and returns the number of \n",
        "  # edges in the graph (in integer). You should not count an edge \n",
        "  # twice if the graph is undirected. For example, in an undirected \n",
        "  # graph G, if two nodes v and u are connected by an edge, this edge\n",
        "  # should only be counted once.\n",
        "\n",
        "  num_edges = pyg_dataset[idx].edge_index.shape[1]\n",
        "  return num_edges\n",
        "\n",
        "idx = 200\n",
        "num_edges = get_graph_num_edges(pyg_dataset, idx)\n",
        "print(f\"Graph with index {idx} has {num_edges} edges\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXa7yIG4E0Fp"
      },
      "source": [
        "## 2 Open Graph Benchmark (OGB) 教程\n",
        "\n",
        "`ogb`包含有很多真实、大规模且多样的图机器学习评测数据集。它里边的数据集会自动下载、处理并用 OGB 自带的数据加载器分割。模型的性能也可以用 OGB 评估器以统一的方式进行评估。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnazPGGAJAZN"
      },
      "source": [
        "### Dataset 和 Data\n",
        "\n",
        "OGB 也支持 `PyG` 的 `Dataset` 和 `Data` 类。现在我们简单地在 `ogbn-arxiv` 数据集上看一下。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpc6bTm3GF02"
      },
      "outputs": [],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "dataset_name = 'ogbn-arxiv'\n",
        "# Load the dataset and transform it to sparse tensor\n",
        "dataset = PygNodePropPredDataset(name=dataset_name, root=root, transform=T.ToSparseTensor())\n",
        "print(f\"The {dataset_name} dataset has {len(dataset)} graph\")\n",
        "\n",
        "# Extract the graph\n",
        "data = dataset[0]\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw0xZJKZI-n3"
      },
      "source": [
        "### 问题4：ogbn-arxiv 图中有多少特征？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP844_nT2ZJl"
      },
      "outputs": [],
      "source": [
        "def graph_num_features(data):\n",
        "  # TODO: Implement this function that takes a PyG data object,\n",
        "  # and returns the number of features in the graph (in integer).\n",
        "\n",
        "  num_features = data.num_features\n",
        "  return num_features\n",
        "\n",
        "num_features = graph_num_features(data)\n",
        "print(f\"The graph has {num_features} features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DP_yEQZ0NVW"
      },
      "source": [
        "## 3 GNN: 结点分类\n",
        "\n",
        "本小节中，我们将用 `PyG` 来搭建第一个图神经网络模型，并在结点分类任务上进行测试。\n",
        "\n",
        "模型将使用 `PyG` 内置的 GCN 算子([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf))，即 `GCNConv` 层直接搭建。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4CcOUEoInjD"
      },
      "source": [
        "### 配置"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DCtgcHpGIpd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "### 数据集加载以及预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ibJ0ieoIwQM"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'ogbn-arxiv'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name, transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "# Make the adjacency matrix to symmetric\n",
        "data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print(f'Device: {device}')\n",
        "\n",
        "data = data.to(device)\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train'].to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "### GCN 模型\n",
        "\n",
        "现在我们开始实现 GCN 模型！请按照下图实现你的 `forward` 方法。\n",
        "\n",
        "![GCN Model](assets/colab2-GCN_model.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgspXTYpNJLA"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "        # TODO: Implement this function that initializes self.convs, \n",
        "        # self.bns, and self.softmax.\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # A list of GCNConv layers\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "\n",
        "        # A list of 1D batch normalization layers\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "\n",
        "        # The log softmax layer\n",
        "        self.softmax = torch.nn.LogSoftmax(dim=-1)\n",
        "\n",
        "        # layers\n",
        "        self.convs.append(GCNConv(input_dim, hidden_dim))\n",
        "        self.convs.extend([GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n",
        "        self.convs.append(GCNConv(hidden_dim, output_dim))\n",
        "        self.bns.extend([torch.nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
        "\n",
        "        # Probability of an element to be zeroed\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Skip classification layer and return node embeddings\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        # TODO: Implement this function that takes the feature tensor x,\n",
        "        # edge_index tensor adj_t and returns the output tensor as\n",
        "        # shown in the figure.\n",
        "\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = conv(x, adj_t)\n",
        "            x = bn(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, training=self.training, p=self.dropout)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        out = x if self.return_embeds else self.softmax(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF1hnHUhO81e"
      },
      "outputs": [],
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    # TODO: Implement this function that trains the model by \n",
        "    # using the given optimizer and loss_fn.\n",
        "    model.train()\n",
        "    loss: torch.Tensor = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t)\n",
        "    pred_y, true_y = out[train_idx], data.y[train_idx].squeeze()\n",
        "    loss = loss_fn(pred_y, true_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJdlrJQhPBsK"
      },
      "outputs": [],
      "source": [
        "# Test function here\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    # TODO: Implement this function that tests the model by \n",
        "    # using the given split_idx and evaluator.\n",
        "    model.eval()\n",
        "\n",
        "    # The output of model on all data\n",
        "    out = model(data.x, data.adj_t)\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7F46xkuLiOL"
      },
      "outputs": [],
      "source": [
        "# Please do not change the args\n",
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 3,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 100,\n",
        "}\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT8RyM2cPGxM"
      },
      "outputs": [],
      "source": [
        "model = GCN(data.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd5O5cnPPdVF"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# reset the parameters to initial random value\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "  result = test(model, data, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duMEg-olLjbJ"
      },
      "source": [
        "### 问题5：你`best_model`在验证集和测试集上的准确率是多少？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqcextqOL2FX"
      },
      "outputs": [],
      "source": [
        "best_result = test(best_model, data, split_idx, evaluator)\n",
        "train_acc, valid_acc, test_acc = best_result\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8pOD6y80TyI"
      },
      "source": [
        "## 4 GNN: 图分类\n",
        "\n",
        "本小节我们会创建一个GNN来做图分类的任务。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRg5VOEdQTa4"
      },
      "source": [
        "### 数据集加载及预处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXb-O5QUIgTH"
      },
      "outputs": [],
      "source": [
        "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
        "from torch_geometric.loader import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Load the dataset \n",
        "dataset = PygGraphPropPredDataset(name='ogbg-molhiv', root='./dataset/')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')\n",
        "\n",
        "split_idx = dataset.get_idx_split()\n",
        "\n",
        "# Check task type\n",
        "print(f'Task type: {dataset.task_type}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cHHbgW1c5hi"
      },
      "outputs": [],
      "source": [
        "# Load the data sets into dataloader\n",
        "# We will train the graph classification task on a batch of 32 graphs\n",
        "# Shuffle the order of graphs for training set\n",
        "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYrSnOj0Y4DK"
      },
      "outputs": [],
      "source": [
        "# Please do not change the args\n",
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 5,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.001,\n",
        "    'epochs': 30,\n",
        "}\n",
        "args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WLhguSTeazy"
      },
      "source": [
        "### 图预测模型\n",
        "\n",
        "现在我们要开始实现GCN图预测模型了！\n",
        "\n",
        "我们将复用已有的GCN模型来生成结点嵌入 `node_embeddings` 并使用全局池化来预测整个图的分类。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_Kq3zyjeZ22"
      },
      "outputs": [],
      "source": [
        "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
        "from torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool\n",
        "\n",
        "### GCN to predict graph property\n",
        "class GCN_Graph(torch.nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, num_layers, dropout, pool=global_mean_pool):\n",
        "        super(GCN_Graph, self).__init__()\n",
        "\n",
        "        # Load encoders for Atoms in molecule graphs\n",
        "        self.node_encoder = AtomEncoder(hidden_dim)\n",
        "\n",
        "        # Node embedding model\n",
        "        # Note that the input_dim and output_dim are set to hidden_dim\n",
        "        self.gnn_node = GCN(hidden_dim, hidden_dim,\n",
        "            hidden_dim, num_layers, dropout, return_embeds=True)\n",
        "\n",
        "        self.pool = global_mean_pool\n",
        "\n",
        "        # Output layer\n",
        "        self.linear = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      self.gnn_node.reset_parameters()\n",
        "      self.linear.reset_parameters()\n",
        "\n",
        "    def forward(self, batched_data):\n",
        "        # TODO: Implement this function that takes the input tensor batched_data,\n",
        "        # returns a batched output tensor for each graph.\n",
        "        x, edge_index, batch = batched_data.x, batched_data.edge_index, batched_data.batch\n",
        "        embed = self.node_encoder(x)\n",
        "\n",
        "        embed = self.gnn_node(embed, edge_index)\n",
        "        embed = self.pool(embed, batch)\n",
        "        out = self.linear(embed)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJjnGuMSbjX0"
      },
      "outputs": [],
      "source": [
        "def train(model, device, data_loader, optimizer, loss_fn):\n",
        "    # TODO: Implement this function that trains the model by \n",
        "    # using the given optimizer and loss_fn.\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(data_loader, desc=\"Iteration\", unit=\"Batch\")):\n",
        "      batch = batch.to(device)\n",
        "\n",
        "      if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
        "          pass\n",
        "      else:\n",
        "        ## ignore nan targets (unlabeled) when computing training loss.\n",
        "        is_labeled = batch.y == batch.y\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        pred_y, true_y = out[is_labeled], batch.y[is_labeled].type(dtype=torch.float32)\n",
        "        loss = loss_fn(pred_y, true_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztPHXq_Gzn7U"
      },
      "outputs": [],
      "source": [
        "# The evaluation function\n",
        "def eval(model, device, loader, evaluator):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\", unit=\"Batch\")):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        if batch.x.shape[0] == 1:\n",
        "            pass\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                pred = model(batch)\n",
        "\n",
        "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
        "            y_pred.append(pred.detach().cpu())\n",
        "\n",
        "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
        "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
        "\n",
        "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
        "\n",
        "    return evaluator.eval(input_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR1wQ4hMZeMw"
      },
      "outputs": [],
      "source": [
        "model = GCN_Graph(args['hidden_dim'],\n",
        "            dataset.num_tasks, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbg-molhiv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJGTNZiuZy0A"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  print('Training...')\n",
        "  loss = train(model, device, train_loader, optimizer, loss_fn)\n",
        "\n",
        "  print('Evaluating...')\n",
        "  train_result = eval(model, device, train_loader, evaluator)\n",
        "  val_result = eval(model, device, valid_loader, evaluator)\n",
        "  test_result = eval(model, device, test_loader, evaluator)\n",
        "\n",
        "  train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uKs6j6t1ah3"
      },
      "source": [
        "### 问题6：你最好的模型 `best_model` 在验证集和测试集上的ROC-AUC是多少？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq5QaG21dOOO"
      },
      "outputs": [],
      "source": [
        "train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
        "valid_acc = eval(best_model, device, valid_loader, evaluator)[dataset.eval_metric]\n",
        "test_acc = eval(best_model, device, test_loader, evaluator)[dataset.eval_metric]\n",
        "\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBi_t8n0iZ4P"
      },
      "source": [
        "### 问题7 (选作)：用 `PyG` 中除了平均池化的其他两个全局池化层试试。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taxEEWyh1_jq"
      },
      "outputs": [],
      "source": [
        "model = GCN_Graph(\n",
        "    args['hidden_dim'],\n",
        "    dataset.num_tasks,\n",
        "    args['num_layers'],\n",
        "    args['dropout'],\n",
        "    pool=global_add_pool).to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  print('Training...')\n",
        "  loss = train(model, device, train_loader, optimizer, loss_fn)\n",
        "\n",
        "  print('Evaluating...')\n",
        "  train_result = eval(model, device, train_loader, evaluator)\n",
        "  val_result = eval(model, device, valid_loader, evaluator)\n",
        "  test_result = eval(model, device, test_loader, evaluator)\n",
        "\n",
        "  train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')\n",
        "\n",
        "train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
        "valid_acc = eval(best_model, device, valid_loader, evaluator)[dataset.eval_metric]\n",
        "test_acc = eval(best_model, device, test_loader, evaluator)[dataset.eval_metric]\n",
        "\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GCN_Graph(\n",
        "    args['hidden_dim'],\n",
        "    dataset.num_tasks,\n",
        "    args['num_layers'],\n",
        "    args['dropout'],\n",
        "    pool=global_max_pool).to(device)\n",
        "\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  print('Training...')\n",
        "  loss = train(model, device, train_loader, optimizer, loss_fn)\n",
        "\n",
        "  print('Evaluating...')\n",
        "  train_result = eval(model, device, train_loader, evaluator)\n",
        "  val_result = eval(model, device, valid_loader, evaluator)\n",
        "  test_result = eval(model, device, test_loader, evaluator)\n",
        "\n",
        "  train_acc, valid_acc, test_acc = train_result[dataset.eval_metric], val_result[dataset.eval_metric], test_result[dataset.eval_metric]\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')\n",
        "\n",
        "train_acc = eval(best_model, device, train_loader, evaluator)[dataset.eval_metric]\n",
        "valid_acc = eval(best_model, device, valid_loader, evaluator)[dataset.eval_metric]\n",
        "test_acc = eval(best_model, device, test_loader, evaluator)[dataset.eval_metric]\n",
        "\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "CS224W - Colab 2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('cs224w')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3d401f214986802b882e8a583f41ee2435f6c7e7677ce5b0132581f351a1fd17"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
